{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learnspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCqeodSaRhwKDH1aTrqwW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshid43016/AnalyticsDataEcosystem/blob/main/Learnspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zTzTbQYDUo7",
        "outputId": "9e669603-eb1f-42ac-d216-da821f0cb1e8"
      },
      "source": [
        "!pwd\n",
        "!ls\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n",
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqGBtgkJDkhK",
        "outputId": "144dc49c-c4e9-4436-a3d8-2a1392d0a983"
      },
      "source": [
        "\n",
        "#!wget https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install pyspark\n",
        "#!tar -xvzf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "#!pip install pypi\n",
        "!pip install findspark"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 65kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=c089e1b2769810cf77519b947122f4288ac971256cbaa9c9a0d3a488d0375eff\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJK-pDUHD0_q",
        "outputId": "769ed1e7-884e-4241-c2cd-f3159f5356b0"
      },
      "source": [
        "!ls\n",
        "\n",
        "!pyspark"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Python 3.7.10 (default, May  3 2021, 02:48:31) \n",
            "[GCC 7.5.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/07/10 15:39:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.7.10 (default, May  3 2021 02:48:31)\n",
            "Spark context Web UI available at http://b25fdb44a8f3:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1625931573660).\n",
            "SparkSession available as 'spark'.\n",
            ">>> help\n",
            "Type help() for interactive help, or help(object) for help about object.\n",
            ">>> help()\n",
            "\n",
            "Welcome to Python 3.7's help utility!\n",
            "\n",
            "If this is your first time using Python, you should definitely check out\n",
            "the tutorial on the Internet at https://docs.python.org/3.7/tutorial/.\n",
            "\n",
            "Enter the name of any module, keyword, or topic to get help on writing\n",
            "Python programs and using Python modules.  To quit this help utility and\n",
            "return to the interpreter, just type \"quit\".\n",
            "\n",
            "To get a list of available modules, keywords, symbols, or topics, type\n",
            "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
            "with a one-line summary of what it does; to list the modules whose name\n",
            "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
            "\n",
            "help> sql\n",
            "Help on package sql:\n",
            "\n",
            "N\bNA\bAM\bME\bE\n",
            "    sql\n",
            "\n",
            "P\bPA\bAC\bCK\bKA\bAG\bGE\bE \b C\bCO\bON\bNT\bTE\bEN\bNT\bTS\bS\n",
            "    column_guesser\n",
            "    connection\n",
            "    magic\n",
            "    parse\n",
            "    run\n",
            "\n",
            "F\bFI\bIL\bLE\bE\n",
            "    /usr/local/lib/python3.7/dist-packages/sql/__init__.py\n",
            "\n",
            "\n",
            "help> quit\n",
            "\n",
            "You are now leaving help and returning to the Python interpreter.\n",
            "If you want to ask for help on a particular object directly from the\n",
            "interpreter, you can type \"help(object)\".  Executing \"help('string')\"\n",
            "has the same effect as typing a particular string at the help> prompt.\n",
            ">>> quit()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v89EW3grE1uA",
        "outputId": "3140f8f1-8409-4e7e-b0cd-99645035e806"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySpark 3.0 Setup on Google Colab\").getOrCreate()\n",
        "print(spark.sparkContext.appName)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PySpark 3.0 Setup on Google Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjKFKUHJFH7G",
        "outputId": "0767b740-55d9-447a-e84d-9355d1af4559"
      },
      "source": [
        "df = spark.read.csv(header=True, sep=\",\", inferSchema=True, path=\"/content/home_insurance.csv\")\n",
        "df.show(2)\n",
        "\n",
        "\n",
        "records_count = df.count()\n",
        "print(\"records_count is %d\" % records_count)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-------------+----------------+-------+--------+------------+-----------------+---------------------+-------------------+-----------+-----------------+--------------------+-------------------+--------------+---------------+----------------+--------------+---------------+----------+-------------+-----------------+------+----------+----------+--------+-----------------+-----------------+--------+------+--------------+-----------+----------+--------------+-------------+---------+--------------+------------+----------+---------+-------------+--------------+-----------------+-------------------+--------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+--------+-------+--------+--------+-------------------+----------+---+-------+\n",
            "|QUOTE_DATE|COVER_START|CLAIM3YEARS|P1_EMP_STATUS|P1_PT_EMP_STATUS|BUS_USE|CLERICAL|AD_BUILDINGS|RISK_RATED_AREA_B|SUM_INSURED_BUILDINGS|NCD_GRANTED_YEARS_B|AD_CONTENTS|RISK_RATED_AREA_C|SUM_INSURED_CONTENTS|NCD_GRANTED_YEARS_C|CONTENTS_COVER|BUILDINGS_COVER|SPEC_SUM_INSURED|SPEC_ITEM_PREM|UNSPEC_HRP_PREM|    P1_DOB|P1_MAR_STATUS|P1_POLICY_REFUSED|P1_SEX|APPR_ALARM|APPR_LOCKS|BEDROOMS|ROOF_CONSTRUCTION|WALL_CONSTRUCTION|FLOODING|LISTED|MAX_DAYS_UNOCC|NEIGH_WATCH|OCC_STATUS|OWNERSHIP_TYPE|PAYING_GUESTS|PROP_TYPE|SAFE_INSTALLED|SEC_DISC_REQ|SUBSIDENCE|YEARBUILT|CAMPAIGN_DESC|PAYMENT_METHOD|PAYMENT_FREQUENCY|LEGAL_ADDON_PRE_REN|LEGAL_ADDON_POST_REN|HOME_EM_ADDON_PRE_REN|HOME_EM_ADDON_POST_REN|GARDEN_ADDON_PRE_REN|GARDEN_ADDON_POST_REN|KEYCARE_ADDON_PRE_REN|KEYCARE_ADDON_POST_REN|HP1_ADDON_PRE_REN|HP1_ADDON_POST_REN|HP2_ADDON_PRE_REN|HP2_ADDON_POST_REN|HP3_ADDON_PRE_REN|HP3_ADDON_POST_REN|MTA_FLAG|MTA_FAP|MTA_APRP|MTA_DATE|LAST_ANN_PREM_GROSS|POL_STATUS|  i| Police|\n",
            "+----------+-----------+-----------+-------------+----------------+-------+--------+------------+-----------------+---------------------+-------------------+-----------+-----------------+--------------------+-------------------+--------------+---------------+----------------+--------------+---------------+----------+-------------+-----------------+------+----------+----------+--------+-----------------+-----------------+--------+------+--------------+-----------+----------+--------------+-------------+---------+--------------+------------+----------+---------+-------------+--------------+-----------------+-------------------+--------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+--------+-------+--------+--------+-------------------+----------+---+-------+\n",
            "|11/22/2007| 22/11/2007|          N|            R|            null|      N|    null|           Y|               19|              1000000|                  7|          Y|                6|               50000|                  7|             Y|              Y|            7500|         44.42|          12.45|15/06/1939|            O|                N|     M|         N|         Y|       3|               11|               15|       Y|     3|             0|          N|        PH|             8|            0|       10|             Y|           Y|         N|     1960|         null|        PureDD|             null|                  N|                   N|                    N|                     N|                   N|                    N|                    N|                     N|                N|                 N|                N|                 N|                N|                 N|       N|   null|    null|    null|             274.81|    Lapsed|  1|P000001|\n",
            "|11/22/2007|   1/1/2008|          N|            E|            null|      Y|       N|           Y|               25|              1000000|                  6|          Y|                9|               50000|                  7|             Y|              Y|               0|           0.0|           24.6|20/05/1970|            M|                N|     M|         N|         N|       3|               11|               15|       Y|     3|             0|          N|        PH|             3|            0|        2|             N|           N|         N|     1960|         null|        PureDD|             null|                  Y|                   Y|                    N|                     N|                   N|                    N|                    N|                     N|                N|                 N|                N|                 N|                N|                 N|       Y| 308.83|   -9.27|    null|             308.83|      Live|  2|P000002|\n",
            "+----------+-----------+-----------+-------------+----------------+-------+--------+------------+-----------------+---------------------+-------------------+-----------+-----------------+--------------------+-------------------+--------------+---------------+----------------+--------------+---------------+----------+-------------+-----------------+------+----------+----------+--------+-----------------+-----------------+--------+------+--------------+-----------+----------+--------------+-------------+---------+--------------+------------+----------+---------+-------------+--------------+-----------------+-------------------+--------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+--------+-------+--------+--------+-------------------+----------+---+-------+\n",
            "only showing top 2 rows\n",
            "\n",
            "records_count is 256136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlXz94mfNgEC",
        "outputId": "8bfee0ee-9631-423e-f91e-8ba237403bb6"
      },
      "source": [
        "#option 1\n",
        "\n",
        "users_list_0 = [\"1|John|London\", \"2|Martin|New York\", \"3|Sam|Sydney\", \"4|Alan|Mexico City\", \"5|Jacob|Florida\"]\n",
        "print(users_list_0)\n",
        "\n",
        "users_list_0_rdd = spark.sparkContext.parallelize(users_list_0)\n",
        "\n",
        "users_list_0_rdd = users_list_0_rdd.map(lambda ele: (int(ele.split('|')[0]), ele.split('|')[1], ele.split('|')[2]))\n",
        "\n",
        "users_df_0 = spark.createDataFrame(users_list_0_rdd)\n",
        "users_df_0.show(10)\n",
        "users_df_0.printSchema()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "users_schema = StructType([\n",
        "                          StructField(\"id\", IntegerType(), True),\n",
        "                          StructField(\"name\", StringType(), True),\n",
        "                          StructField(\"city\", StringType(), True)])\n",
        "\n",
        "users_df_01 = spark.createDataFrame(users_list_0_rdd, schema=users_schema)\n",
        "users_df_01.show(10)\n",
        "users_df_01.printSchema()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1|John|London', '2|Martin|New York', '3|Sam|Sydney', '4|Alan|Mexico City', '5|Jacob|Florida']\n",
            "+---+------+-----------+\n",
            "| _1|    _2|         _3|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            " |-- _3: string (nullable = true)\n",
            "\n",
            "+---+------+-----------+\n",
            "| id|  name|       city|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFrGvqmLNmuL",
        "outputId": "fbd2a9a1-743a-4d0b-afe5-9d7d8fd502e2"
      },
      "source": [
        "#option 2\n",
        "\n",
        "users_list_1 = [(1, \"John\", \"London\"), (2, \"Martin\", \"New York\"), (3, \"Sam\", \"Sydney\"), (4, \"Alan\", \"Mexico City\"), (5, \"Jacob\", \"Florida\")]\n",
        "print(users_list_1)\n",
        "\n",
        "users_df_1 = spark.createDataFrame(users_list_1)\n",
        "users_df_1.show(10)\n",
        "users_df_1.printSchema()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "users_schema = StructType([\n",
        "                          StructField(\"id\", IntegerType(), True),\n",
        "                          StructField(\"name\", StringType(), True),\n",
        "                          StructField(\"city\", StringType(), True)])\n",
        "\n",
        "users_df_11 = spark.createDataFrame(users_list_1, schema=users_schema)\n",
        "users_df_11.show(10)\n",
        "users_df_11.printSchema()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 'John', 'London'), (2, 'Martin', 'New York'), (3, 'Sam', 'Sydney'), (4, 'Alan', 'Mexico City'), (5, 'Jacob', 'Florida')]\n",
            "+---+------+-----------+\n",
            "| _1|    _2|         _3|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            " |-- _3: string (nullable = true)\n",
            "\n",
            "+---+------+-----------+\n",
            "| id|  name|       city|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iczdz9kxQPo-",
        "outputId": "c85b3103-8f9f-45cb-8c02-8b8aeb9814e4"
      },
      "source": [
        "#Option 3 \n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "users_list_2 = [Row(1, \"John\", \"London\"), Row(2, \"Martin\", \"New York\"), Row(3, \"Sam\", \"Sydney\"), Row(4, \"Alan\", \"Mexico City\"), Row(5, \"Jacob\", \"Florida\")]\n",
        "print(users_list_2)\n",
        "\n",
        "users_df_2 = spark.createDataFrame(users_list_2)\n",
        "users_df_2.show(10)\n",
        "users_df_2.printSchema()\n",
        "\n",
        "users_df_21 = spark.createDataFrame(users_list_2, schema=users_schema)\n",
        "users_df_21.show(10)\n",
        "users_df_21.printSchema()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Row(1, 'John', 'London')>, <Row(2, 'Martin', 'New York')>, <Row(3, 'Sam', 'Sydney')>, <Row(4, 'Alan', 'Mexico City')>, <Row(5, 'Jacob', 'Florida')>]\n",
            "+---+------+-----------+\n",
            "| _1|    _2|         _3|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            " |-- _3: string (nullable = true)\n",
            "\n",
            "+---+------+-----------+\n",
            "| id|  name|       city|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL0bSv0iQZjK",
        "outputId": "e7218eb7-47c5-44ab-ef91-37d976a76533"
      },
      "source": [
        "#option 4\n",
        "\n",
        "users_list_3 = [{\"id\": 1, \"name\": \"John\", \"city\": \"London\"}, \n",
        "                {\"id\": 2, \"name\": \"Martin\", \"city\": \"New York\"}, \n",
        "                {\"id\": 3, \"name\": \"Sam\", \"city\": \"Sydney\"}, \n",
        "                {\"id\": 4, \"name\": \"Alan\", \"city\": \"Mexico City\"}, \n",
        "                {\"id\": 5, \"name\": \"Jacob\", \"city\": \"Florida\"}]\n",
        "print(users_list_3)\n",
        "\n",
        "users_df_3 = spark.createDataFrame(users_list_3)\n",
        "users_df_3.show(10)\n",
        "users_df_3.printSchema()\n",
        "\n",
        "users_df_31 = spark.createDataFrame(users_list_3, schema=users_schema)\n",
        "users_df_31.show(10)\n",
        "users_df_31.printSchema()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'id': 1, 'name': 'John', 'city': 'London'}, {'id': 2, 'name': 'Martin', 'city': 'New York'}, {'id': 3, 'name': 'Sam', 'city': 'Sydney'}, {'id': 4, 'name': 'Alan', 'city': 'Mexico City'}, {'id': 5, 'name': 'Jacob', 'city': 'Florida'}]\n",
            "+-----------+---+------+\n",
            "|       city| id|  name|\n",
            "+-----------+---+------+\n",
            "|     London|  1|  John|\n",
            "|   New York|  2|Martin|\n",
            "|     Sydney|  3|   Sam|\n",
            "|Mexico City|  4|  Alan|\n",
            "|    Florida|  5| Jacob|\n",
            "+-----------+---+------+\n",
            "\n",
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+------+-----------+\n",
            "| id|  name|       city|\n",
            "+---+------+-----------+\n",
            "|  1|  John|     London|\n",
            "|  2|Martin|   New York|\n",
            "|  3|   Sam|     Sydney|\n",
            "|  4|  Alan|Mexico City|\n",
            "|  5| Jacob|    Florida|\n",
            "+---+------+-----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS1KdsGPQeHg"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW0FqYdwWv4-",
        "outputId": "79348b20-189b-4a84-81fe-36107a7eb430"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Create DataFrame from JSON File in PySpark 3.0\").getOrCreate()\n",
        "print(spark.sparkContext.appName)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create DataFrame from JSON File in PySpark 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGBT7PNqW2Za"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "user_schema = StructType([\n",
        "                     StructField(\"userId\", IntegerType(), True),\n",
        "                     StructField(\"firstName\", StringType(), True),\n",
        "                     StructField(\"lastName\", StringType(), True),\n",
        "                     StructField(\"phonenumber\", StringType(), True)\n",
        "])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLlwB5SiXPaB",
        "outputId": "aa2c1e9d-43af-4103-c061-cbfa57da5d28"
      },
      "source": [
        "json_file_path_1 = \"/content/user_details1.json\"\n",
        "\n",
        "\n",
        "df = spark.read.json(path=json_file_path_1)\n",
        "\n",
        "df.show()\n",
        "\n",
        "json_file_path_2 = \"/content/user_details.json\"\n",
        "\n",
        "\n",
        "df2 = spark.read.json(path=json_file_path_2 ,  multiLine=True) \n",
        "\n",
        "df2.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+--------------+------+\n",
            "|firstName|lastName|   phoneNumber|userId|\n",
            "+---------+--------+--------------+------+\n",
            "|    Raman|    kaur|+1 657 897 234|     1|\n",
            "|  Raman12|  kaur12|+1 127 817 224|     2|\n",
            "+---------+--------+--------------+------+\n",
            "\n",
            "+---------+--------+--------------+------+\n",
            "|firstName|lastName|   phoneNumber|userId|\n",
            "+---------+--------+--------------+------+\n",
            "|    Raman|    kaur|+1 657 897 234|     1|\n",
            "|  Raman12|  kaur12|+1 127 817 224|     2|\n",
            "+---------+--------+--------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}